{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e940423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [03:08<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pdf saved in ./litqa_pdfs/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "jsonl_path = \"litqa-v2-public.jsonl\"\n",
    "output_dir = Path(\"./litqa_pdfs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_dois(jsonl_file):\n",
    "    dois = set()\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            sources = entry.get(\"sources\", [])\n",
    "            for src in sources:\n",
    "                if \"doi.org\" in src:\n",
    "                    dois.add(src.strip())\n",
    "    return list(dois)\n",
    "\n",
    "async def fetch_pdf(session, doi_url, save_path):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    if save_path.exists():\n",
    "        return \"exists\"\n",
    "\n",
    "    try:\n",
    "        async with session.get(doi_url, headers=headers, timeout=30) as resp:\n",
    "            text = await resp.text()\n",
    "            if \".pdf\" in text:\n",
    "                start = text.find(\"https://\")\n",
    "                end = text.find(\".pdf\", start)\n",
    "                if start != -1 and end != -1:\n",
    "                    pdf_url = text[start:end + 4]\n",
    "\n",
    "                    async with session.get(pdf_url, headers=headers, timeout=30) as pdf_resp:\n",
    "                        content = await pdf_resp.read()\n",
    "                        with open(save_path, \"wb\") as f:\n",
    "                            f.write(content)\n",
    "                        return \"downloaded\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {e}\"\n",
    "\n",
    "    return \"not_found\"\n",
    "\n",
    "async def download_all(dois):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        results = []\n",
    "        for doi in tqdm(dois):\n",
    "            filename = doi.split(\"/\")[-1] + \".pdf\"\n",
    "            path = output_dir / filename\n",
    "            result = await fetch_pdf(session, doi, path)\n",
    "            results.append((doi, result))\n",
    "        return results\n",
    "\n",
    "dois = extract_dois(jsonl_path)\n",
    "nest_asyncio.apply()\n",
    "results = await download_all(dois)\n",
    "\n",
    "with open(\"download_log.txt\", \"w\") as log:\n",
    "    for doi, status in results:\n",
    "        log.write(f\"{doi}\\t{status}\\n\")\n",
    "\n",
    "print(\"All pdf saved in ./litqa_pdfs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d27b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:26<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading 191 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# File paths\n",
    "jsonl_path = \"litqa-v2-public.jsonl\"\n",
    "output_dir = Path(\"./litqa_pdfs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Map DOI or URL to a safe filename\n",
    "def safe_filename(doi_or_url: str) -> str:\n",
    "    return urllib.parse.quote_plus(doi_or_url) + \".pdf\"\n",
    "\n",
    "# Extract all unique source DOIs from jsonl file\n",
    "def extract_dois(jsonl_file):\n",
    "    dois = set()\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            for src in entry.get(\"sources\", []):\n",
    "                if \"doi.org\" in src or \"arxiv.org\" in src:\n",
    "                    dois.add(src.strip())\n",
    "    return sorted(list(dois))\n",
    "\n",
    "# Try downloading PDF from a given DOI/URL\n",
    "async def fetch_pdf(session, doi_url, save_path):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    if save_path.exists() and save_path.stat().st_size > 10_000:\n",
    "        return \"exists\"\n",
    "\n",
    "    try:\n",
    "        # Special case: arXiv\n",
    "        if \"arxiv.org\" in doi_url:\n",
    "            arxiv_id = doi_url.split(\"/\")[-1]\n",
    "            pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "            async with session.get(pdf_url, headers=headers, timeout=30) as resp:\n",
    "                if resp.status == 200 and \"application/pdf\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                    content = await resp.read()\n",
    "                    with open(save_path, \"wb\") as f:\n",
    "                        f.write(content)\n",
    "                    return \"arxiv_downloaded\"\n",
    "                return f\"arxiv_fail_{resp.status}\"\n",
    "\n",
    "        # Fallback: generic DOI\n",
    "        async with session.get(doi_url, headers=headers, timeout=30, allow_redirects=True) as resp:\n",
    "            if resp.status == 200 and \"application/pdf\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                content = await resp.read()\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(content)\n",
    "                return \"pdf_downloaded\"\n",
    "            else:\n",
    "                return f\"not_pdf_or_fail_{resp.status}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"error: {str(e)}\"\n",
    "\n",
    "# Main loop for downloading all papers\n",
    "async def download_all(dois):\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for doi in tqdm(dois):\n",
    "            filename = safe_filename(doi)\n",
    "            path = output_dir / filename\n",
    "            result = await fetch_pdf(session, doi, path)\n",
    "            results.append((doi, filename, result))\n",
    "    return results\n",
    "\n",
    "# Run\n",
    "dois = extract_dois(jsonl_path)\n",
    "nest_asyncio.apply()\n",
    "results = await download_all(dois)\n",
    "\n",
    "# Save download log\n",
    "with open(\"download_log.txt\", \"w\") as f:\n",
    "    for doi, fname, status in results:\n",
    "        f.write(f\"{doi}\\t{fname}\\t{status}\\n\")\n",
    "\n",
    "print(f\"Finished downloading {len(results)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a27d4f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [02:59<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Downloaded 58 PDFs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# Configuration\n",
    "jsonl_path = \"litqa-v2-public.jsonl\"\n",
    "output_dir = Path(\"./litqa_pdfs\")\n",
    "email = \"xc392@cam.ac.uk\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helpers\n",
    "def safe_filename(doi: str) -> str:\n",
    "    return urllib.parse.quote_plus(doi) + \".pdf\"\n",
    "\n",
    "def extract_dois(jsonl_file):\n",
    "    dois = set()\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            for src in entry.get(\"sources\", []):\n",
    "                if \"doi.org\" in src:\n",
    "                    dois.add(src.strip().split(\"doi.org/\")[-1])\n",
    "    return sorted(list(dois))\n",
    "\n",
    "# Download via Unpaywall\n",
    "async def get_pdf_url_from_unpaywall(session, doi: str) -> str | None:\n",
    "    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
    "    try:\n",
    "        async with session.get(api_url, timeout=15) as resp:\n",
    "            if resp.status != 200:\n",
    "                return None\n",
    "            data = await resp.json()\n",
    "            pdf_info = data.get(\"best_oa_location\", {})\n",
    "            return pdf_info.get(\"url_for_pdf\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "async def download_pdf(session, url: str, save_path: Path) -> bool:\n",
    "    try:\n",
    "        async with session.get(url, timeout=30) as resp:\n",
    "            if resp.status == 200 and \"application/pdf\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                content = await resp.read()\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(content)\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# Main download loop\n",
    "async def download_all_via_unpaywall(dois):\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for doi in tqdm(dois):\n",
    "            filename = safe_filename(doi)\n",
    "            save_path = output_dir / filename\n",
    "\n",
    "            if save_path.exists() and save_path.stat().st_size > 10_000:\n",
    "                results.append((doi, \"exists\"))\n",
    "                continue\n",
    "\n",
    "            pdf_url = await get_pdf_url_from_unpaywall(session, doi)\n",
    "            if not pdf_url:\n",
    "                results.append((doi, \"no_pdf\"))\n",
    "                continue\n",
    "\n",
    "            success = await download_pdf(session, pdf_url, save_path)\n",
    "            results.append((doi, \"downloaded\" if success else \"fail_download\"))\n",
    "    return results\n",
    "\n",
    "# Run\n",
    "dois = extract_dois(jsonl_path)\n",
    "nest_asyncio.apply()\n",
    "results = await download_all_via_unpaywall(dois)\n",
    "\n",
    "# Save log\n",
    "with open(\"download_log_unpaywall.txt\", \"w\") as f:\n",
    "    for doi, status in results:\n",
    "        f.write(f\"{doi}\\t{status}\\n\")\n",
    "\n",
    "print(f\"Finished. Downloaded {sum(1 for _, s in results if s == 'downloaded')} PDFs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef951ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json5\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# Config\n",
    "split_file = \"2024-10-16_litqa2-splits.json5\"\n",
    "output_dir = Path(\"./litqa_pdfs\")\n",
    "email = \"xc392@cam.ac.uk\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parse JSON5\n",
    "with open(split_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    split_data = json5.load(f)\n",
    "\n",
    "eval_dois = sorted(set(split_data.get(\"eval\", {}).get(\"dois\", [])))\n",
    "print(f\"✅ Found {len(eval_dois)} eval DOIs\")\n",
    "\n",
    "# Unpaywall Downloader\n",
    "async def get_pdf_url_from_unpaywall(session, doi: str) -> str | None:\n",
    "    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
    "    try:\n",
    "        async with session.get(api_url, timeout=15) as resp:\n",
    "            if resp.status != 200:\n",
    "                return None\n",
    "            data = await resp.json()\n",
    "            pdf_info = data.get(\"best_oa_location\", {})\n",
    "            return pdf_info.get(\"url_for_pdf\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "async def download_pdf(session, url: str, save_path: Path) -> bool:\n",
    "    try:\n",
    "        async with session.get(url, timeout=30) as resp:\n",
    "            if resp.status == 200 and \"application/pdf\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                content = await resp.read()\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(content)\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "async def download_eval_dois(dois):\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for doi in tqdm(dois):\n",
    "            filename = urllib.parse.quote_plus(doi) + \".pdf\"\n",
    "            save_path = output_dir / filename\n",
    "\n",
    "            if save_path.exists() and save_path.stat().st_size > 10_000:\n",
    "                results.append((doi, \"exists\"))\n",
    "                continue\n",
    "\n",
    "            pdf_url = await get_pdf_url_from_unpaywall(session, doi)\n",
    "            if not pdf_url:\n",
    "                results.append((doi, \"no_pdf\"))\n",
    "                continue\n",
    "\n",
    "            success = await download_pdf(session, pdf_url, save_path)\n",
    "            results.append((doi, \"downloaded\" if success else \"fail_download\"))\n",
    "    return results\n",
    "\n",
    "# Run It\n",
    "nest_asyncio.apply()\n",
    "results = await download_eval_dois(eval_dois)\n",
    "\n",
    "# Log It\n",
    "with open(\"download_eval_log.txt\", \"w\") as log_file:\n",
    "    for doi, status in results:\n",
    "        log_file.write(f\"{doi}\\t{status}\\n\")\n",
    "\n",
    "print(f\"Finished downloading. {sum(1 for _, s in results if s == 'downloaded')} PDFs downloaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96f60d",
   "metadata": {},
   "source": [
    "If download all the pdfs of the eval dois, there are 5456 to be downloaded, that's too many! So we only download those DOIs that are actually cited by the question used by eval split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab67b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 DOIs for eval questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:30<00:00,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading. 12 PDFs downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import json5\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# Input files\n",
    "split_file = \"2024-10-16_litqa2-splits.json5\"\n",
    "jsonl_file = \"litqa-v2-public.jsonl\"\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"./litqa_pdfs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Contact email for Unpaywall\n",
    "email = \"xc392@cam.ac.uk\"\n",
    "\n",
    "# Extract eval question IDs from JSON5 split file\n",
    "with open(split_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    split_data = json5.load(f)\n",
    "eval_ids = set(split_data.get(\"eval\", {}).get(\"question_ids\", []))\n",
    "\n",
    "# Extract DOIs from jsonl questions matching eval_ids\n",
    "eval_dois = set()\n",
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        if entry.get(\"id\") in eval_ids:\n",
    "            for src in entry.get(\"sources\", []):\n",
    "                if \"doi.org\" in src:\n",
    "                    doi = src.strip().split(\"doi.org/\")[-1]\n",
    "                    eval_dois.add(doi)\n",
    "\n",
    "print(f\"Found {len(eval_dois)} DOIs for eval questions\")\n",
    "\n",
    "# Use Unpaywall API to resolve free PDF URLs\n",
    "async def get_pdf_url_from_unpaywall(session, doi: str) -> str | None:\n",
    "    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
    "    try:\n",
    "        async with session.get(api_url, timeout=15) as resp:\n",
    "            if resp.status != 200:\n",
    "                return None\n",
    "            data = await resp.json()\n",
    "            pdf_info = data.get(\"best_oa_location\", {})\n",
    "            return pdf_info.get(\"url_for_pdf\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Download the PDF file from a given URL\n",
    "async def download_pdf(session, url: str, save_path: Path) -> bool:\n",
    "    try:\n",
    "        async with session.get(url, timeout=30) as resp:\n",
    "            if resp.status == 200 and \"application/pdf\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                content = await resp.read()\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(content)\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# Batch download all DOIs using aiohttp\n",
    "async def download_eval_dois(dois):\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for doi in tqdm(sorted(dois)):\n",
    "            filename = urllib.parse.quote_plus(doi) + \".pdf\"\n",
    "            save_path = output_dir / filename\n",
    "\n",
    "            if save_path.exists() and save_path.stat().st_size > 10_000:\n",
    "                results.append((doi, \"exists\"))\n",
    "                continue\n",
    "\n",
    "            pdf_url = await get_pdf_url_from_unpaywall(session, doi)\n",
    "            if not pdf_url:\n",
    "                results.append((doi, \"no_pdf\"))\n",
    "                continue\n",
    "\n",
    "            success = await download_pdf(session, pdf_url, save_path)\n",
    "            results.append((doi, \"downloaded\" if success else \"fail_download\"))\n",
    "    return results\n",
    "\n",
    "# Run the download pipeline\n",
    "nest_asyncio.apply()\n",
    "results = await download_eval_dois(eval_dois)\n",
    "\n",
    "# Log download results\n",
    "with open(\"download_eval_log.txt\", \"w\") as log_file:\n",
    "    for doi, status in results:\n",
    "        log_file.write(f\"{doi}\\t{status}\\n\")\n",
    "\n",
    "print(f\"Finished downloading. {sum(1 for _, s in results if s == 'downloaded')} PDFs downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f64af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated manifest with 14 entries.\n"
     ]
    }
   ],
   "source": [
    "# Get manifest.json\n",
    "import urllib.parse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_dir = Path(\"litqa_pdfs\")\n",
    "manifest = {}\n",
    "\n",
    "for pdf_path in pdf_dir.glob(\"*.pdf\"):\n",
    "    decoded_doi = urllib.parse.unquote_plus(pdf_path.stem)\n",
    "    manifest[decoded_doi] = {\n",
    "        \"doi\": decoded_doi,\n",
    "        \"file_path\": str(pdf_path.name)\n",
    "    }\n",
    "\n",
    "with open(\"manifest.jsonl\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Generated manifest with {len(manifest)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f208b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import json5\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# Input files\n",
    "split_file = \"2024-10-16_litqa2-splits.json5\"\n",
    "jsonl_file = \"litqa-v2-public.jsonl\"\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"./litqa_pdfs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output file for manual download list\n",
    "manual_json_path = \"manual_download_needed.json\"\n",
    "\n",
    "# Contact email for Unpaywall API\n",
    "email = \"xc392@cam.ac.uk\"\n",
    "\n",
    "# Load train + eval question IDs from JSON5 split file\n",
    "with open(split_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    split_data = json5.load(f)\n",
    "train_ids = set(split_data.get(\"train\", {}).get(\"question_ids\", []))\n",
    "eval_ids = set(split_data.get(\"eval\", {}).get(\"question_ids\", []))\n",
    "combined_ids = train_ids | eval_ids\n",
    "print(f\"Total questions to process (train + eval): {len(combined_ids)}\")\n",
    "\n",
    "# Extract DOIs from jsonl for the selected questions\n",
    "combined_doi_map = {}  # key: question_id, value: set of DOIs\n",
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        qid = entry.get(\"id\")\n",
    "        if qid in combined_ids:\n",
    "            for src in entry.get(\"sources\", []):\n",
    "                if \"doi.org\" in src:\n",
    "                    doi = src.strip().split(\"doi.org/\")[-1]\n",
    "                    combined_doi_map.setdefault(qid, set()).add(doi)\n",
    "\n",
    "# Flatten to unique DOIs\n",
    "combined_dois = set(doi for dois in combined_doi_map.values() for doi in dois)\n",
    "print(f\"Found {len(combined_dois)} unique DOIs for train + eval questions\")\n",
    "\n",
    "# Resolve a DOI to a PDF URL via Unpaywall\n",
    "async def get_pdf_url_from_unpaywall(session, doi: str) -> str | None:\n",
    "    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
    "    try:\n",
    "        async with session.get(api_url, timeout=15) as resp:\n",
    "            if resp.status != 200:\n",
    "                return None\n",
    "            data = await resp.json()\n",
    "            pdf_info = data.get(\"best_oa_location\", {})\n",
    "            return pdf_info.get(\"url_for_pdf\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Download a PDF from a URL to a specified path\n",
    "async def download_pdf(session, url: str, save_path: Path) -> bool:\n",
    "    try:\n",
    "        async with session.get(url, timeout=30) as resp:\n",
    "            if resp.status == 200 and \"application/pdf\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                content = await resp.read()\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(content)\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# Download all DOIs and track failures for manual handling\n",
    "async def download_all_dois(dois, doi_to_qids):\n",
    "    results = []\n",
    "    manual_entries = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for doi in tqdm(sorted(dois)):\n",
    "            filename = urllib.parse.quote_plus(doi) + \".pdf\"\n",
    "            save_path = output_dir / filename\n",
    "\n",
    "            if save_path.exists() and save_path.stat().st_size > 10_000:\n",
    "                results.append((doi, \"exists\"))\n",
    "                continue\n",
    "\n",
    "            pdf_url = await get_pdf_url_from_unpaywall(session, doi)\n",
    "            if not pdf_url:\n",
    "                results.append((doi, \"no_pdf\"))\n",
    "                for qid in doi_to_qids.get(doi, []):\n",
    "                    manual_entries.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"doi\": doi,\n",
    "                        \"download_url\": None\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            success = await download_pdf(session, pdf_url, save_path)\n",
    "            status = \"downloaded\" if success else \"fail_download\"\n",
    "            results.append((doi, status))\n",
    "            if not success:\n",
    "                for qid in doi_to_qids.get(doi, []):\n",
    "                    manual_entries.append({\n",
    "                        \"question_id\": qid,\n",
    "                        \"doi\": doi,\n",
    "                        \"download_url\": pdf_url\n",
    "                    })\n",
    "    return results, manual_entries\n",
    "\n",
    "# Build DOI to question ID reverse mapping\n",
    "doi_to_qids = {}\n",
    "for qid, dois in combined_doi_map.items():\n",
    "    for doi in dois:\n",
    "        doi_to_qids.setdefault(doi, []).append(qid)\n",
    "\n",
    "# Run the download process\n",
    "nest_asyncio.apply()\n",
    "results, manual_entries = await download_all_dois(combined_dois, doi_to_qids)\n",
    "\n",
    "# Save log file\n",
    "with open(\"download_eval_log.txt\", \"w\") as log_file:\n",
    "    for doi, status in results:\n",
    "        log_file.write(f\"{doi}\\t{status}\\n\")\n",
    "\n",
    "# Save failed or missing download information\n",
    "with open(manual_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manual_entries, f, indent=2)\n",
    "\n",
    "print(f\"Finished downloading. {sum(1 for _, s in results if s == 'downloaded')} PDFs downloaded.\")\n",
    "print(f\"Manual download list saved to {manual_json_path} with {len(manual_entries)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5fa448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [02:43<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total attempted: 205\n",
      "Downloaded: 58\n",
      "Failed (manual): 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# File paths\n",
    "jsonl_file = \"litqa-v2-public.jsonl\"\n",
    "output_dir = Path(\"./litqa_pdfs\")\n",
    "manual_output_file = \"manual_download_needed.json\"\n",
    "\n",
    "# Contact email for Unpaywall\n",
    "email = \"xc392@cam.ac.uk\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extract all (question_id, doi) pairs from litqa-v2-public.jsonl\n",
    "question_doi_pairs = []\n",
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        qid = entry.get(\"id\")\n",
    "        for src in entry.get(\"sources\", []):\n",
    "            if \"doi.org\" in src:\n",
    "                doi = src.strip().split(\"doi.org/\")[-1]\n",
    "                question_doi_pairs.append((qid, doi))\n",
    "\n",
    "# Use Unpaywall API to resolve PDF URLs\n",
    "async def get_pdf_url_from_unpaywall(session, doi: str) -> str | None:\n",
    "    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
    "    try:\n",
    "        async with session.get(api_url, timeout=15) as resp:\n",
    "            if resp.status != 200:\n",
    "                return None\n",
    "            data = await resp.json()\n",
    "            pdf_info = data.get(\"best_oa_location\", {})\n",
    "            return pdf_info.get(\"url_for_pdf\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Download the PDF from resolved URL\n",
    "async def download_pdf(session, url: str, save_path: Path) -> bool:\n",
    "    try:\n",
    "        async with session.get(url, timeout=30) as resp:\n",
    "            if resp.status == 200 and \"application/pdf\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                content = await resp.read()\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(content)\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# Batch download all DOIs, record manual entries if failed\n",
    "async def download_all():\n",
    "    results = []\n",
    "    manual_entries = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for qid, doi in tqdm(question_doi_pairs):\n",
    "            filename = urllib.parse.quote_plus(doi) + \".pdf\"\n",
    "            save_path = output_dir / filename\n",
    "\n",
    "            if save_path.exists() and save_path.stat().st_size > 10_000:\n",
    "                results.append((qid, doi, \"exists\"))\n",
    "                continue\n",
    "\n",
    "            pdf_url = await get_pdf_url_from_unpaywall(session, doi)\n",
    "            if not pdf_url:\n",
    "                results.append((qid, doi, \"no_pdf\"))\n",
    "                manual_entries.append({\n",
    "                    \"question_id\": qid,\n",
    "                    \"doi\": doi,\n",
    "                    \"download_url\": None\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            success = await download_pdf(session, pdf_url, save_path)\n",
    "            status = \"downloaded\" if success else \"fail_download\"\n",
    "            results.append((qid, doi, status))\n",
    "\n",
    "            if not success:\n",
    "                manual_entries.append({\n",
    "                    \"question_id\": qid,\n",
    "                    \"doi\": doi,\n",
    "                    \"download_url\": pdf_url\n",
    "                })\n",
    "    return results, manual_entries\n",
    "\n",
    "# Run the download pipeline\n",
    "nest_asyncio.apply()\n",
    "results, manual_entries = asyncio.run(download_all())\n",
    "\n",
    "# Save failed entries for manual download\n",
    "with open(manual_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manual_entries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Optional: print summary\n",
    "print(f\"Total attempted: {len(results)}\")\n",
    "print(f\"Downloaded: {sum(1 for _, _, s in results if s == 'downloaded')}\")\n",
    "print(f\"Failed (manual): {len(manual_entries)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "20114a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1126%2Fscience.abk2432.pdf\n",
      "https://doi.org/10.1126/science.abk2432\n"
     ]
    }
   ],
   "source": [
    "doi = '10.1126/science.abk2432'\n",
    "from urllib.parse import quote_plus\n",
    "filename = quote_plus(doi) + '.pdf'\n",
    "print(filename)\n",
    "print(\"https://doi.org/\" + doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed5589",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
