{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "> This tutorial is available as a Jupyter notebook [here](https://github.com/Future-House/paper-qa/blob/main/docs/tutorials/settings_tutorial.ipynb).\n",
    "\n",
    "This tutorial aims to show how to use the `Settings` class to configure `PaperQA`.\n",
    "Firstly, we will be using `OpenAI` and `Anthropic` models, so we need to set the `OPENAI_API_KEY` and `ANTHROPIC_API_KEY` environment variables.\n",
    "We will use both models to make it clear when `paperqa` agent is using either one or the other.\n",
    "We use `python-dotenv` to load the environment variables from a `.env` file.\n",
    "Hence, our first step is to create a `.env` file and install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "# Create .env file with OpenAI API and Anthropic API keys\n",
    "# Replace <your-openai-api-key> and <your-anthropic-api-key> with your actual API keys\n",
    "!echo \"OPENAI_API_KEY=<your-openai-api-key>\" > .env # fmt: skip\n",
    "!echo \"ANTHROPIC_API_KEY=<your-anthropic-api-key>\" >> .env # fmt: skip\n",
    "\n",
    "!uv pip install -q nest-asyncio python-dotenv aiohttp fhlmi \"paper-qa[local]\"\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You have set the following environment variables:\")\n",
    "print(\n",
    "    f\"OPENAI_API_KEY:    {'is set' if os.environ['OPENAI_API_KEY'] else 'is not set'}\"\n",
    ")\n",
    "print(\n",
    "    f\"ANTHROPIC_API_KEY: {'is set' if os.environ['ANTHROPIC_API_KEY'] else 'is not set'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `lmi` package to get the model names and the `.papers` directory to save documents we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmi import CommonLLMNames\n",
    "\n",
    "llm_openai = CommonLLMNames.OPENAI_TEST.value\n",
    "llm_anthropic = CommonLLMNames.ANTHROPIC_TEST.value\n",
    "\n",
    "# Create the `papers` directory if it doesn't exist\n",
    "os.makedirs(\"papers\", exist_ok=True)\n",
    "\n",
    "# Download the paper from arXiv and save it to the `papers` directory\n",
    "url = \"https://arxiv.org/pdf/2407.01603\"\n",
    "async with aiohttp.ClientSession() as session, session.get(url, timeout=60) as response:\n",
    "    content = await response.read()\n",
    "    with open(\"papers/2407.01603.pdf\", \"wb\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Settings` class is used to configure the PaperQA settings.\n",
    "Official documentation can be found [here](https://github.com/Future-House/paper-qa?tab=readme-ov-file#settings-cheatsheet) and the open source code can be found [here](https://github.com/Future-House/paper-qa/blob/main/paperqa/settings.py).\n",
    "\n",
    "Here is a basic example of how to use the `Settings` class. We will be unnecessarily verbose for the sake of clarity. Please notice that most of the settings are optional and the defaults are good for most cases. Refer to the [descriptions of each setting](https://github.com/Future-House/paper-qa/blob/main/paperqa/settings.py) for more information.\n",
    "\n",
    "Within this `Settings` object, I'd like to discuss specifically how the llms are configured and how `paperqa` looks for papers.\n",
    "\n",
    "A common source of confusion is that multiple `llms` are used in paperqa. We have `llm`, `summary_llm`, `agent_llm`, and `embedding`. Hence, if `llm` is set to an `Anthropic` model, `summary_llm` and `agent_llm` will still require a `OPENAI_API_KEY`, since `OpenAI` models are the default.\n",
    "\n",
    "Among the objects that use `llms` in `paperqa`, we have `llm`, `summary_llm`, `agent_llm`, and `embedding`:\n",
    "\n",
    "- `llm`: Main LLM used by the agent to reason about the question, extract metadata from documents, etc.\n",
    "- `summary_llm`: LLM used to summarize the papers.\n",
    "- `agent_llm`: LLM used to answer questions and select tools.\n",
    "- `embedding`: Embedding model used to embed the papers.\n",
    "\n",
    "Let's see some examples around this concept. First, we define the settings with `llm` set to an `OpenAI` model. Please notice this is not an complete list of settings. But take your time to read through this `Settings` class and all customization that can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from paperqa.prompts import (\n",
    "    CONTEXT_INNER_PROMPT,\n",
    "    CONTEXT_OUTER_PROMPT,\n",
    "    citation_prompt,\n",
    "    default_system_prompt,\n",
    "    env_reset_prompt,\n",
    "    env_system_prompt,\n",
    "    qa_prompt,\n",
    "    select_paper_prompt,\n",
    "    structured_citation_prompt,\n",
    "    summary_json_prompt,\n",
    "    summary_json_system_prompt,\n",
    "    summary_prompt,\n",
    ")\n",
    "from paperqa.settings import (\n",
    "    AgentSettings,\n",
    "    AnswerSettings,\n",
    "    IndexSettings,\n",
    "    ParsingSettings,\n",
    "    PromptSettings,\n",
    "    Settings,\n",
    ")\n",
    "\n",
    "settings = Settings(\n",
    "    llm=llm_openai,\n",
    "    llm_config={\n",
    "        \"model_list\": [\n",
    "            {\n",
    "                \"model_name\": llm_openai,\n",
    "                \"litellm_params\": {\n",
    "                    \"model\": llm_openai,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"max_tokens\": 4096,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"rate_limit\": {\n",
    "            llm_openai: \"30000 per 1 minute\",\n",
    "        },\n",
    "    },\n",
    "    summary_llm=llm_openai,\n",
    "    summary_llm_config={\n",
    "        \"rate_limit\": {\n",
    "            llm_openai: \"30000 per 1 minute\",\n",
    "        },\n",
    "    },\n",
    "    embedding=\"text-embedding-3-small\",\n",
    "    embedding_config={},\n",
    "    temperature=0.1,\n",
    "    batch_size=1,\n",
    "    verbosity=1,\n",
    "    manifest_file=None,\n",
    "    paper_directory=pathlib.Path.cwd().joinpath(\"papers\"),\n",
    "    index_directory=pathlib.Path.cwd().joinpath(\"papers/index\"),\n",
    "    answer=AnswerSettings(\n",
    "        evidence_k=10,\n",
    "        evidence_detailed_citations=True,\n",
    "        evidence_retrieval=True,\n",
    "        evidence_summary_length=\"about 100 words\",\n",
    "        evidence_skip_summary=False,\n",
    "        answer_max_sources=5,\n",
    "        max_answer_attempts=None,\n",
    "        answer_length=\"about 200 words, but can be longer\",\n",
    "        max_concurrent_requests=10,\n",
    "    ),\n",
    "    parsing=ParsingSettings(\n",
    "        chunk_size=5000,\n",
    "        overlap=250,\n",
    "        citation_prompt=citation_prompt,\n",
    "        structured_citation_prompt=structured_citation_prompt,\n",
    "    ),\n",
    "    prompts=PromptSettings(\n",
    "        summary=summary_prompt,\n",
    "        qa=qa_prompt,\n",
    "        select=select_paper_prompt,\n",
    "        pre=None,\n",
    "        post=None,\n",
    "        system=default_system_prompt,\n",
    "        use_json=True,\n",
    "        summary_json=summary_json_prompt,\n",
    "        summary_json_system=summary_json_system_prompt,\n",
    "        context_outer=CONTEXT_OUTER_PROMPT,\n",
    "        context_inner=CONTEXT_INNER_PROMPT,\n",
    "    ),\n",
    "    agent=AgentSettings(\n",
    "        agent_llm=llm_openai,\n",
    "        agent_llm_config={\n",
    "            \"model_list\": [\n",
    "                {\n",
    "                    \"model_name\": llm_openai,\n",
    "                    \"litellm_params\": {\n",
    "                        \"model\": llm_openai,\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "            \"rate_limit\": {\n",
    "                llm_openai: \"30000 per 1 minute\",\n",
    "            },\n",
    "        },\n",
    "        agent_prompt=env_reset_prompt,\n",
    "        agent_system_prompt=env_system_prompt,\n",
    "        search_count=8,\n",
    "        index=IndexSettings(\n",
    "            paper_directory=pathlib.Path.cwd().joinpath(\"papers\"),\n",
    "            index_directory=pathlib.Path.cwd().joinpath(\"papers/index\"),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is evident, `Paperqa` is absolutely customizable. And here we reinterate that despite this possible fine customization, the defaults are good for most cases. Although, the user is welcome to explore the settings and customize the `paperqa` to their needs.\n",
    "\n",
    "We also set settings.verbosity to 1, which will print the agent configuration. Feel free to set it to 0 to silence the logging after your first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import ask\n",
    "\n",
    "response = ask(\n",
    "    \"What are the most relevant language models used for chemistry?\", settings=settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which probably worked fine. Let's now try to remove `OPENAI_API_KEY` and run again the same question with the same settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "print(\"You have set the following environment variables:\")\n",
    "print(\n",
    "    f\"OPENAI_API_KEY:    {'is set' if os.environ['OPENAI_API_KEY'] else 'is not set'}\"\n",
    ")\n",
    "print(\n",
    "    f\"ANTHROPIC_API_KEY: {'is set' if os.environ['ANTHROPIC_API_KEY'] else 'is not set'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask(\n",
    "    \"What are the most relevant language models used for chemistry?\", settings=settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would obviously fail. We don't have a valid `OPENAI_API_KEY`, so the agent will not be able to use `OpenAI` models. Let's change it to an `Anthropic` model and see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.llm = llm_anthropic\n",
    "settings.llm_config = {\n",
    "    \"model_list\": [\n",
    "        {\n",
    "            \"model_name\": llm_anthropic,\n",
    "            \"litellm_params\": {\n",
    "                \"model\": llm_anthropic,\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 512,\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"rate_limit\": {\n",
    "        llm_anthropic: \"30000 per 1 minute\",\n",
    "    },\n",
    "}\n",
    "settings.summary_llm = llm_anthropic\n",
    "settings.summary_llm_config = {\n",
    "    \"rate_limit\": {\n",
    "        llm_anthropic: \"30000 per 1 minute\",\n",
    "    },\n",
    "}\n",
    "settings.agent = AgentSettings(\n",
    "    agent_llm=llm_anthropic,\n",
    "    agent_llm_config={\n",
    "        \"rate_limit\": {\n",
    "            llm_anthropic: \"30000 per 1 minute\",\n",
    "        },\n",
    "    },\n",
    "    index=IndexSettings(\n",
    "        paper_directory=pathlib.Path.cwd().joinpath(\"papers\"),\n",
    "        index_directory=pathlib.Path.cwd().joinpath(\"papers/index\"),\n",
    "    ),\n",
    ")\n",
    "settings.embedding = \"st-multi-qa-MiniLM-L6-cos-v1\"\n",
    "response = ask(\n",
    "    \"What are the most relevant language models used for chemistry?\", settings=settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the agent is able to use `Anthropic` models only and although we don't have a valid `OPENAI_API_KEY`, the question is answered because the agent will not use `OpenAI` models. See that we also changed the `embedding` because it was using `text-embedding-3-small` by default, which is a `OpenAI` model. `Paperqa` implements a few embedding models. Please refer to the [documentation](https://github.com/Future-House/paper-qa?tab=readme-ov-file#embedding-model) for more information.\n",
    "\n",
    "Notice that we redefined `settings.agent.paper_directory` and `settings.agent.index` settings. `Paperqa` actually uses the setting from `settings.agent`. However, for convenience, we implemented an alias in `settings.paper_directory` and `settings.index_directory`.\n",
    "\n",
    "In addition, notice that this is a very verbose example for the sake of clarity. We could have just set only the llms names and used default settings for the rest:\n",
    "\n",
    "```python\n",
    "llm_anthropic_config = {\n",
    "    \"model_list\": [{\n",
    "            \"model_name\": llm_anthropic,\n",
    "    }]\n",
    "}\n",
    "\n",
    "settings.llm = llm_anthropic\n",
    "settings.llm_config = llm_anthropic_config\n",
    "settings.summary_llm = llm_anthropic\n",
    "settings.summary_llm_config = llm_anthropic_config\n",
    "settings.agent = AgentSettings(\n",
    "    agent_llm=llm_anthropic,\n",
    "    agent_llm_config=llm_anthropic_config,\n",
    "    index=IndexSettings(\n",
    "        paper_directory=pathlib.Path.cwd().joinpath(\"papers\"),\n",
    "        index_directory=pathlib.Path.cwd().joinpath(\"papers/index\"),\n",
    "    ),\n",
    ")\n",
    "settings.embedding = \"st-multi-qa-MiniLM-L6-cos-v1\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The output\n",
    "\n",
    "`Paperqa` returns a `PQASession` object, which contains not only the answer but also all the information gatheres to answer the questions. We recommend printing the `PQASession` object (`print(response.session)`) to understand the information it contains. Let's check the `PQASession` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Let's examine the PQASession object returned by paperqa:\\n\")\n",
    "\n",
    "print(f\"Status: {response.status.value}\")\n",
    "\n",
    "print(\"1. Question asked:\")\n",
    "print(f\"{response.session.question}\\n\")\n",
    "\n",
    "print(\"2. Answer provided:\")\n",
    "print(f\"{response.session.answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the answer, the `PQASession` object contains all the references and contexts used to generate the answer.\n",
    "\n",
    "Because `paperqa` splits the documents into chunks, each chunk is a valid reference. You can see that it also references the page where the context was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3. References cited:\")\n",
    "print(f\"{response.session.references}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, `PQASession.session.contexts` contains the contexts used to generate the answer. Each context has a score, which is the similarity between the question and the context.\n",
    "`Paperqa` uses this score to choose what contexts is more relevant to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4. Contexts used to generate the answer:\")\n",
    "print(\n",
    "    \"These are the relevant text passages that were retrieved and used to formulate the answer:\"\n",
    ")\n",
    "for i, ctx in enumerate(response.session.contexts, 1):\n",
    "    print(f\"\\nContext {i}:\")\n",
    "    print(f\"Source: {ctx.text.name}\")\n",
    "    print(f\"Content: {ctx.context}\")\n",
    "    print(f\"Score: {ctx.score}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
