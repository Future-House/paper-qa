{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring PaperQA2 with LFRQA\n",
    "> This tutorial is available as a Jupyter notebook [here](https://github.com/Future-House/paper-qa/blob/main/docs/tutorials/running_on_lfrqa.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **LFRQA dataset** was introduced in the paper [_RAG-QA Arena: Evaluating Domain Robustness for Long-Form Retrieval-Augmented Question Answering_](https://arxiv.org/pdf/2407.13998). It features **1,404 science questions** (along with other categories) that have been human-annotated with answers. This tutorial walks through the process of setting up the dataset for use and benchmarking.\n",
    "\n",
    "## Download the Annotations\n",
    "\n",
    "First, we need to obtain the annotated dataset from the official repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    20  100    20    0     0    148      0 --:--:-- --:--:-- --:--:--   149\n",
      "curl: (6) Could not resolve host: annotations_science_with_citation.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Create a new directory for the dataset\n",
    "!mkdir -p data/rag-qa-benchmarking\n",
    "\n",
    "# Get the annotated questions\n",
    "!curl https://raw.githubusercontent.com/awslabs/rag-qa-arena/refs/heads/main/data/\\\n",
    "annotations_science_with_citation.jsonl \\\n",
    "-o data/rag-qa-benchmarking/annotations_science_with_citation.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Download the Robust-QA Documents\n",
    "\n",
    "LFRQA is built upon **Robust-QA**, so we must download the relevant documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3410M  100 3410M    0     0  5159k      0  0:11:16  0:11:16 --:--:-- 5124k-:--  0:19:25 2997k  0  0:09:05  0:00:13  0:08:52 5211k     0  0:09:34  0:00:18  0:09:16 5225k    0  0:09:55  0:00:24  0:09:31 5220k 0     0  5402k      0  0:10:46  0:00:49  0:09:57 3916k5330k      0  0:10:55  0:01:17  0:09:38 5191k9  0:09:03 5131k:08:36 5209k04  0:02:34  0:08:30 5189k0     0  5254k      0  0:11:04  0:02:44  0:08:20 5235k0k   0     0  5252k      0  0:11:04  0:02:51  0:08:13 5223k5249k      0  0:11:05  0:03:03  0:08:02 5204k03:10  0:07:55 5199k 0:11:05  0:03:14  0:07:51 5200k 0  0:11:07  0:04:01  0:07:06 5116k 5211k      0  0:11:10  0:04:56  0:06:14 5134k:06:09 5122k      0  0:11:11  0:05:30  0:05:41 5123k  0:05:21 5185k  0:05:19 5154k5118k1  0:04:51 5120k 0     0  5190k      0  0:11:12  0:06:24  0:04:48 5111k4k06:50  0:04:23 5120k 0  5172k      0  0:11:15  0:08:28  0:02:47 5094k:05 5111k5165k      0  0:11:16  0:09:57  0:01:19 5125k64k      0  0:11:16  0:10:02  0:01:14 5105k  0  0:11:16  0:10:32  0:00:44 5117k:47  0:00:29 5117k   0  5160k      0  0:11:16  0:11:05  0:00:11 5120k   0  0:11:16  0:11:16 --:--:-- 5109k\n",
      "x lotte/\n",
      "x lotte/science/\n",
      "x lotte/science/test/\n",
      "x lotte/science/test/questions.search.tsv\n",
      "x lotte/science/test/questions.forum.tsv\n",
      "x lotte/science/test/collection.tsv\n",
      "x lotte/science/test/qas.forum.jsonl\n",
      "x lotte/science/test/metadata.jsonl\n",
      "x lotte/science/test/qas.search.jsonl\n",
      "x lotte/science/dev/\n",
      "x lotte/science/dev/questions.search.tsv\n",
      "x lotte/science/dev/questions.forum.tsv\n",
      "x lotte/science/dev/collection.tsv\n",
      "x lotte/science/dev/qas.forum.jsonl\n",
      "x lotte/science/dev/metadata.jsonl\n",
      "x lotte/science/dev/qas.search.jsonl\n",
      "x lotte/writing/\n",
      "x lotte/writing/test/\n",
      "x lotte/writing/test/questions.search.tsv\n",
      "x lotte/writing/test/questions.forum.tsv\n",
      "x lotte/writing/test/collection.tsv\n",
      "x lotte/writing/test/qas.forum.jsonl\n",
      "x lotte/writing/test/metadata.jsonl\n",
      "x lotte/writing/test/qas.search.jsonl\n",
      "x lotte/writing/dev/\n",
      "x lotte/writing/dev/questions.search.tsv\n",
      "x lotte/writing/dev/questions.forum.tsv\n",
      "x lotte/writing/dev/collection.tsv\n",
      "x lotte/writing/dev/qas.forum.jsonl\n",
      "x lotte/writing/dev/metadata.jsonl\n",
      "x lotte/writing/dev/qas.search.jsonl\n",
      "x lotte/recreation/\n",
      "x lotte/recreation/test/\n",
      "x lotte/recreation/test/questions.search.tsv\n",
      "x lotte/recreation/test/questions.forum.tsv\n",
      "x lotte/recreation/test/collection.tsv\n",
      "x lotte/recreation/test/qas.forum.jsonl\n",
      "x lotte/recreation/test/metadata.jsonl\n",
      "x lotte/recreation/test/qas.search.jsonl\n",
      "x lotte/recreation/dev/\n",
      "x lotte/recreation/dev/questions.search.tsv\n",
      "x lotte/recreation/dev/questions.forum.tsv\n",
      "x lotte/recreation/dev/collection.tsv\n",
      "x lotte/recreation/dev/qas.forum.jsonl\n",
      "x lotte/recreation/dev/metadata.jsonl\n",
      "x lotte/recreation/dev/qas.search.jsonl\n",
      "x lotte/lifestyle/\n",
      "x lotte/lifestyle/test/\n",
      "x lotte/lifestyle/test/questions.search.tsv\n",
      "x lotte/lifestyle/test/questions.forum.tsv\n",
      "x lotte/lifestyle/test/collection.tsv\n",
      "x lotte/lifestyle/test/qas.forum.jsonl\n",
      "x lotte/lifestyle/test/metadata.jsonl\n",
      "x lotte/lifestyle/test/qas.search.jsonl\n",
      "x lotte/lifestyle/dev/\n",
      "x lotte/lifestyle/dev/questions.search.tsv\n",
      "x lotte/lifestyle/dev/questions.forum.tsv\n",
      "x lotte/lifestyle/dev/collection.tsv\n",
      "x lotte/lifestyle/dev/qas.forum.jsonl\n",
      "x lotte/lifestyle/dev/metadata.jsonl\n",
      "x lotte/lifestyle/dev/qas.search.jsonl\n",
      "x lotte/evaluate_lotte_rankings.py\n",
      "x lotte/technology/\n",
      "x lotte/technology/test/\n",
      "x lotte/technology/test/questions.search.tsv\n",
      "x lotte/technology/test/questions.forum.tsv\n",
      "x lotte/technology/test/collection.tsv\n",
      "x lotte/technology/test/qas.forum.jsonl\n",
      "x lotte/technology/test/metadata.jsonl\n",
      "x lotte/technology/test/qas.search.jsonl\n",
      "x lotte/technology/dev/\n",
      "x lotte/technology/dev/questions.search.tsv\n",
      "x lotte/technology/dev/questions.forum.tsv\n",
      "x lotte/technology/dev/collection.tsv\n",
      "x lotte/technology/dev/qas.forum.jsonl\n",
      "x lotte/technology/dev/metadata.jsonl\n",
      "x lotte/technology/dev/qas.search.jsonl\n",
      "x lotte/pooled/\n",
      "x lotte/pooled/test/\n",
      "x lotte/pooled/test/questions.search.tsv\n",
      "x lotte/pooled/test/questions.forum.tsv\n",
      "x lotte/pooled/test/collection.tsv\n",
      "x lotte/pooled/test/qas.forum.jsonl\n",
      "x lotte/pooled/test/qas.search.jsonl\n",
      "x lotte/pooled/dev/\n",
      "x lotte/pooled/dev/questions.search.tsv\n",
      "x lotte/pooled/dev/questions.forum.tsv\n",
      "x lotte/pooled/dev/collection.tsv\n",
      "x lotte/pooled/dev/qas.forum.jsonl\n",
      "x lotte/pooled/dev/qas.search.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Download the Lotte dataset, which includes the required documents\n",
    "!curl https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/lotte.tar.gz --output lotte.tar.gz\n",
    "\n",
    "# Extract the dataset\n",
    "!tar -xvzf lotte.tar.gz\n",
    "\n",
    "# Move the science test collection to our dataset folder\n",
    "!cp lotte/science/test/collection.tsv ./data/rag-qa-benchmarking/science_test_collection.tsv\n",
    "\n",
    "# Clean up unnecessary files\n",
    "!rm lotte.tar.gz\n",
    "!rm -rf lotte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, refer to the original paper: [_RAG-QA Arena: Evaluating Domain Robustness for Long-Form Retrieval-Augmented Question Answering_](https://arxiv.org/pdf/2407.13998)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "We now load the documents into a pandas dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load questions and answers dataset\n",
    "rag_qa_benchmarking_dir = os.path.join(\"data\", \"rag-qa-benchmarking\")\n",
    "\n",
    "# Load documents dataset\n",
    "lfrqa_docs_df = pd.read_csv(\n",
    "    os.path.join(rag_qa_benchmarking_dir, \"science_test_collection.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    names=[\"doc_id\", \"doc_text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the Documents to Use\n",
    "RobustQA consists on 1.7M documents. Hence, it takes around 3 hours to build the whole index.\n",
    "\n",
    "To run a test, we can use 1% of the dataset. This will be accomplished by selecting the first 1% available documents and the questions referent to these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16940 out of 1694041 documents\n"
     ]
    }
   ],
   "source": [
    "proportion_to_use = 1 / 100\n",
    "amount_of_docs_to_use = int(len(lfrqa_docs_df) * proportion_to_use)\n",
    "print(f\"Using {amount_of_docs_to_use} out of {len(lfrqa_docs_df)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Document Files\n",
    "We now create the document directory and store each document as a separate text file, so that paperqa can build the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.01%\n",
      "Progress: 5.01%\n",
      "Progress: 10.01%\n",
      "Progress: 15.01%\n",
      "Progress: 20.01%\n",
      "Progress: 25.01%\n",
      "Progress: 30.01%\n",
      "Progress: 35.01%\n",
      "Progress: 40.01%\n",
      "Progress: 45.01%\n",
      "Progress: 50.01%\n",
      "Progress: 55.01%\n",
      "Progress: 60.01%\n",
      "Progress: 65.01%\n",
      "Progress: 70.01%\n",
      "Progress: 75.01%\n",
      "Progress: 80.01%\n",
      "Progress: 85.01%\n",
      "Progress: 90.01%\n",
      "Progress: 95.01%\n"
     ]
    }
   ],
   "source": [
    "partial_docs = lfrqa_docs_df.head(amount_of_docs_to_use)\n",
    "lfrqa_directory = os.path.join(rag_qa_benchmarking_dir, \"lfrqa\")\n",
    "os.makedirs(\n",
    "    os.path.join(lfrqa_directory, \"science_docs_for_paperqa\", \"files\"), exist_ok=True\n",
    ")\n",
    "\n",
    "for i, row in partial_docs.iterrows():\n",
    "    doc_id = row[\"doc_id\"]\n",
    "    doc_text = row[\"doc_text\"]\n",
    "\n",
    "    with open(\n",
    "        os.path.join(\n",
    "            lfrqa_directory, \"science_docs_for_paperqa\", \"files\", f\"{doc_id}.txt\"\n",
    "        ),\n",
    "        \"w\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f:\n",
    "        f.write(doc_text)\n",
    "\n",
    "    if i % int(len(partial_docs) * 0.05) == 0:\n",
    "        progress = (i + 1) / len(partial_docs)\n",
    "        print(f\"Progress: {progress:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Manifest File\n",
    "The **manifest file** keeps track of document metadata for the dataset. We need to fill some fields so that paperqa doesn’t try to get metadata using llm calls. This will make the indexing process faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = partial_docs.copy()\n",
    "manifest[\"file_location\"] = manifest[\"doc_id\"].apply(lambda x: f\"files/{x}.txt\")\n",
    "manifest[\"doi\"] = \"\"\n",
    "manifest[\"title\"] = manifest[\"doc_id\"]\n",
    "manifest[\"key\"] = manifest[\"doc_id\"]\n",
    "manifest[\"docname\"] = manifest[\"doc_id\"]\n",
    "manifest[\"citation\"] = \"_\"\n",
    "manifest = manifest.drop(columns=[\"doc_id\", \"doc_text\"])\n",
    "manifest.to_csv(\n",
    "    os.path.join(lfrqa_directory, \"science_docs_for_paperqa\", \"manifest.csv\"),\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Save Questions\n",
    "Finally, we load the questions and filter them to ensure we only include questions that reference the selected documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 62 questions\n"
     ]
    }
   ],
   "source": [
    "questions_df = pd.read_json(\n",
    "    os.path.join(rag_qa_benchmarking_dir, \"annotations_science_with_citation.jsonl\"),\n",
    "    lines=True,\n",
    ")\n",
    "partial_questions = questions_df[\n",
    "    questions_df.gold_doc_ids.apply(\n",
    "        lambda ids: all(_id < amount_of_docs_to_use for _id in ids)\n",
    "    )\n",
    "]\n",
    "partial_questions.to_csv(\n",
    "    os.path.join(lfrqa_directory, \"questions.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "print(\"Using\", len(partial_questions), \"questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install paperqa\n",
    "From now on, we will be using the paperqa library, so we need to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paper-qa in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (5.12.0)\n",
      "Requirement already satisfied: PyMuPDF>=1.24.12 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (1.25.3)\n",
      "Requirement already satisfied: aiohttp>=3.10.6 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (3.11.12)\n",
      "Requirement already satisfied: anyio in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (4.8.0)\n",
      "Requirement already satisfied: fh-llm-client>=0.0.11 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (0.0.11)\n",
      "Requirement already satisfied: fhaviary>=0.14 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from fhaviary[llm]>=0.14->paper-qa) (0.18.2.dev48+g0aba2db)\n",
      "Requirement already satisfied: html2text in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (2024.2.26)\n",
      "Requirement already satisfied: httpx in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (0.28.1)\n",
      "Requirement already satisfied: numpy in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (2.2.3)\n",
      "Requirement already satisfied: pybtex in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (0.24.0)\n",
      "Requirement already satisfied: pydantic-settings in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (2.7.1)\n",
      "Requirement already satisfied: pydantic>=2.10.1,~=2.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (2.10.1)\n",
      "Requirement already satisfied: rich in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (13.9.4)\n",
      "Requirement already satisfied: setuptools in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (75.8.0)\n",
      "Requirement already satisfied: tantivy in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (0.22.0)\n",
      "Requirement already satisfied: tenacity in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.4.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from paper-qa) (0.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from aiohttp>=3.10.6->paper-qa) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from aiohttp>=3.10.6->paper-qa) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from aiohttp>=3.10.6->paper-qa) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from aiohttp>=3.10.6->paper-qa) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from aiohttp>=3.10.6->paper-qa) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from aiohttp>=3.10.6->paper-qa) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from aiohttp>=3.10.6->paper-qa) (1.18.3)\n",
      "Requirement already satisfied: coredis in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from fh-llm-client>=0.0.11->paper-qa) (4.18.0)\n",
      "Requirement already satisfied: limits in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from fh-llm-client>=0.0.11->paper-qa) (4.0.1)\n",
      "Requirement already satisfied: litellm in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from fh-llm-client>=0.0.11->paper-qa) (1.61.8)\n",
      "Requirement already satisfied: docstring_parser>=0.16 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from fhaviary>=0.14->fhaviary[llm]>=0.14->paper-qa) (0.16)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from pydantic>=2.10.1,~=2.0->paper-qa) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from pydantic>=2.10.1,~=2.0->paper-qa) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from pydantic>=2.10.1,~=2.0->paper-qa) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from tiktoken>=0.4.0->paper-qa) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from tiktoken>=0.4.0->paper-qa) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from anyio->paper-qa) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from anyio->paper-qa) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from httpx->paper-qa) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from httpx->paper-qa) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->paper-qa) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=3.01 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from pybtex->paper-qa) (6.0.2)\n",
      "Requirement already satisfied: latexcodec>=1.0.4 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from pybtex->paper-qa) (3.0.0)\n",
      "Requirement already satisfied: six in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from pybtex->paper-qa) (1.17.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from pydantic-settings->paper-qa) (1.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from rich->paper-qa) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from rich->paper-qa) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->paper-qa) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.4.0->paper-qa) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.4.0->paper-qa) (2.3.0)\n",
      "Requirement already satisfied: async_timeout<6,>4 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from coredis->fh-llm-client>=0.0.11->paper-qa) (5.0.1)\n",
      "Requirement already satisfied: deprecated>=1.2 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from coredis->fh-llm-client>=0.0.11->paper-qa) (1.2.18)\n",
      "Requirement already satisfied: packaging<25,>=21 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from coredis->fh-llm-client>=0.0.11->paper-qa) (24.2)\n",
      "Requirement already satisfied: pympler<2,>1 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from coredis->fh-llm-client>=0.0.11->paper-qa) (1.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.1.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from coredis->fh-llm-client>=0.0.11->paper-qa) (1.17.2)\n",
      "Requirement already satisfied: click in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from litellm->fh-llm-client>=0.0.11->paper-qa) (8.1.8)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from litellm->fh-llm-client>=0.0.11->paper-qa) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from litellm->fh-llm-client>=0.0.11->paper-qa) (3.1.5)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from litellm->fh-llm-client>=0.0.11->paper-qa) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.61.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from litellm->fh-llm-client>=0.0.11->paper-qa) (1.63.2)\n",
      "Requirement already satisfied: tokenizers in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from litellm->fh-llm-client>=0.0.11->paper-qa) (0.21.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm->fh-llm-client>=0.0.11->paper-qa) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm->fh-llm-client>=0.0.11->paper-qa) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->fh-llm-client>=0.0.11->paper-qa) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->fh-llm-client>=0.0.11->paper-qa) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->fh-llm-client>=0.0.11->paper-qa) (0.22.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from openai>=1.61.0->litellm->fh-llm-client>=0.0.11->paper-qa) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from openai>=1.61.0->litellm->fh-llm-client>=0.0.11->paper-qa) (0.8.2)\n",
      "Requirement already satisfied: tqdm>4 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from openai>=1.61.0->litellm->fh-llm-client>=0.0.11->paper-qa) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from tokenizers->litellm->fh-llm-client>=0.0.11->paper-qa) (0.28.1)\n",
      "Requirement already satisfied: filelock in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm->fh-llm-client>=0.0.11->paper-qa) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/joaquinpolonuer/Documents/benchmark_with_pqa_lib/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm->fh-llm-client>=0.0.11->paper-qa) (2025.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install paper-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the Documents\n",
    "\n",
    "Now we will build an index for the LFRQA documents. The index is a **Tantivy index**, which is a fast, full-text search engine library written in Rust. Tantivy is designed to handle large datasets efficiently, making it ideal for searching through a vast collection of papers or documents.\n",
    "\n",
    "Feel free to adjust the concurrency settings as you like. Because we defined a manifest, we don’t need any API keys for building this index because we don't discern any citation metadata, but you do need LLM API keys to answer questions.\n",
    "\n",
    "Remember that this process is quick for small portions of the dataset, but can take around 3 hours for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the line above to handle async code within a notebook.\n",
    "\n",
    "However, to improve compatibility and speed up the indexing process, we strongly recommend running the following code in a separate `.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from paperqa import Settings\n",
    "from paperqa.agents import build_index\n",
    "from paperqa.settings import AgentSettings, IndexSettings, ParsingSettings\n",
    "\n",
    "settings = Settings(\n",
    "    agent=AgentSettings(\n",
    "        index=IndexSettings(\n",
    "            name=\"lfrqa_science_index\",\n",
    "            paper_directory=os.path.join(\n",
    "                \"data\", \"rag-qa-benchmarking\", \"lfrqa\", \"science_docs_for_paperqa\"\n",
    "            ),\n",
    "            index_directory=os.path.join(\n",
    "                \"data\", \"rag-qa-benchmarking\", \"lfrqa\", \"science_docs_for_paperqa_index\"\n",
    "            ),\n",
    "            manifest_file=\"manifest.csv\",\n",
    "            concurrency=10_000,\n",
    "            batch_size=10_000,\n",
    "        )\n",
    "    ),\n",
    "    parsing=ParsingSettings(\n",
    "        use_doc_details=False,\n",
    "        defer_embedding=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "build_index(settings=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this runs, you will have an index ready to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark!\n",
    "After you have built the index, you are ready to run the benchmark. We advice running this in a separate `.py` file.\n",
    "\n",
    "To run this, you will need to have the [`ldp`](https://github.com/Future-House/ldp) and [`fhaviary[lfrqa]`](https://github.com/Future-House/aviary/blob/main/packages/lfrqa/README.md#installation) packages installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.8)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/joaquinpolonuer/Documents/paper-qa/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install ldp \"fhaviary[lfrqa]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from aviary.envs.lfrqa import LFRQAQuestion, LFRQATaskDataset\n",
    "from ldp.agent import SimpleAgent\n",
    "from ldp.alg.runners import Evaluator, EvaluatorConfig\n",
    "\n",
    "from paperqa import Settings\n",
    "from paperqa.settings import AgentSettings, IndexSettings\n",
    "\n",
    "log_results_dir = os.path.join(\"data\", \"rag-qa-benchmarking\", \"results\")\n",
    "os.makedirs(log_results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "async def log_evaluation_to_json(lfrqa_question_evaluation: dict) -> None:  # noqa: RUF029\n",
    "    json_path = os.path.join(\n",
    "        log_results_dir, f\"{lfrqa_question_evaluation['qid']}.json\"\n",
    "    )\n",
    "    with open(json_path, \"w\") as f:  # noqa: ASYNC230\n",
    "        json.dump(lfrqa_question_evaluation, f, indent=2)\n",
    "\n",
    "\n",
    "async def evaluate() -> None:\n",
    "    settings = Settings(\n",
    "        agent=AgentSettings(\n",
    "            index=IndexSettings(\n",
    "                name=\"lfrqa_science_index\",\n",
    "                paper_directory=os.path.join(\n",
    "                    \"data\", \"rag-qa-benchmarking\", \"lfrqa\", \"science_docs_for_paperqa\"\n",
    "                ),\n",
    "                index_directory=os.path.join(\n",
    "                    \"data\",\n",
    "                    \"rag-qa-benchmarking\",\n",
    "                    \"lfrqa\",\n",
    "                    \"science_docs_for_paperqa_index\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data: list[LFRQAQuestion] = [\n",
    "        LFRQAQuestion(**row)\n",
    "        for row in pd.read_csv(\n",
    "            os.path.join(\"data\", \"rag-qa-benchmarking\", \"lfrqa\", \"questions.csv\")\n",
    "        )[[\"qid\", \"question\", \"answer\", \"gold_doc_ids\"]].to_dict(orient=\"records\")\n",
    "    ]\n",
    "\n",
    "    dataset = LFRQATaskDataset(\n",
    "        data=data,\n",
    "        settings=settings,\n",
    "        evaluation_callback=log_evaluation_to_json,\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        config=EvaluatorConfig(batch_size=3),\n",
    "        agent=SimpleAgent(),\n",
    "        dataset=dataset,\n",
    "    )\n",
    "    await evaluator.evaluate()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(evaluate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this, you can find the results in the `data/rag-qa-benchmarking/results` folder. Here is an example of how to read them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "json_files = glob.glob(os.path.join(rag_qa_benchmarking_dir, \"results\", \"*.json\"))\n",
    "\n",
    "data = []\n",
    "for file in json_files:\n",
    "    with open(file) as f:\n",
    "        json_data = json.load(f)\n",
    "        json_data[\"qid\"] = file.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "        data.append(json_data)\n",
    "\n",
    "results_df = pd.DataFrame(data).set_index(\"qid\")\n",
    "results_df[\"winner\"].value_counts(normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
